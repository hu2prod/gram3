tokenizer = new Tokenizer
tokenizer.parser_list.push (new Token_parser 'dollar_id', /^\$[_a-z0-9]+/i)
tokenizer.parser_list.push (new Token_parser 'hash_id', /^\#[_a-z0-9]+/i)
tokenizer.parser_list.push (new Token_parser 'pass_id', /^\@[_a-z0-9]+/i)
tokenizer.parser_list.push (new Token_parser 'id', /^[_a-z][_a-z0-9]*/i)
tokenizer.parser_list.push (new Token_parser '_bin_op', /// ^ (
  (&&?|\|\|?|[-+*/])|
  <>|[<>!=]=|<|>
) ///)
tokenizer.parser_list.push (new Token_parser '_pre_op', /^!/)
# tokenizer.parser_list.push (new Token_parser 'assign_bin_op', /^(&&?|\|\|?|[-+])?=/)
tokenizer.parser_list.push (new Token_parser 'bracket', /^[\[\]\(\)\{\}]/)
tokenizer.parser_list.push (new Token_parser 'delimiter', /^[:.]/)



string_regex_craft = ///
    \\[^xu] |               # x and u are case sensitive while hex letters are not
    \\x[0-9a-fA-F]{2} |     # Hexadecimal escape sequence
    \\u(?:
      [0-9a-fA-F]{4} |      # Unicode escape sequence
      \{(?:
        [0-9a-fA-F]{1,5} |  # Unicode code point escapes from 0 to FFFFF
        10[0-9a-fA-F]{4}    # Unicode code point escapes from 100000 to 10FFFF
      )\}
    )
///.toString().replace(/\//g,'')
single_quoted_regex_craft = ///
  (?:
    [^\\] |
    #{string_regex_craft}
  )*?
///.toString().replace(/\//g,'')
tokenizer.parser_list.push (new Token_parser 'string_literal_singleq'      , /// ^  ' #{single_quoted_regex_craft} '    ///)
double_quoted_regexp_craft = ///
  (?:
    [^\\#] |
    \#(?!\{) |
    #{string_regex_craft}
  )*?
///.toString().replace(/\//g,'')
tokenizer.parser_list.push (new Token_parser 'string_literal_doubleq'      , /// ^  " #{double_quoted_regexp_craft} "    ///)

tokenizer.parser_list.push (new Token_parser 'number', /^[0-9]+/)